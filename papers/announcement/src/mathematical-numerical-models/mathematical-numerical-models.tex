\section{Mathematical and Numerical Models}\label{sec:MNmodels}

In many (most) circumstances, the solution of equation \ref{eq:IVP} cannot be computed in a closed, exact form (even if it exists and is unique) due to the complexity and nature of the residuals functions, that is often non linear. Consequently, the problem is often solved relying on a numerical approach: the solution of system \ref{eq:IVP} at a time $t^n$, namely $U(t^n)$, is approximated by a subsequent time-marching approximations $U_0=u_0 \rightarrow u_1 \rightarrow u_2 \rightarrow ... \rightarrow u_N\approx U(t^n)$ where the relation $u_i \rightarrow u_{i+1}$ implies a \emph{stepping, numerical integration} from the time $t^i$ to time $t^{i+1}$ and $N$ is the total number of numerical time steps necessary to evolve the initial conditions toward the searched solution $U(t^n)$. To this aim, many numerical schemes have been devised. Notably, the numerical schemes of practical usefulness must posses some necessary proprieties such as \emph{consistency} and \emph{stability} to ensure that the numerical approximation \emph{converges} to the exact solution as the numerical time step tends to zero. A detailed discussion of these details is out the scope of the present work and is omitted. Here, we briefly recall some classifications necessary to introduce the schemes implemented into the FOODIE library.

A non comprehensive classification of the most widely used schemes could distinguish between \emph{multi-step} versus \emph{one-step} schemes and between \emph{explicit} versus \emph{implicit} schemes.

Essentially, the multi-step schemes have been developed to obtain an accurate approximation of the subsequent numerical steps using the informations contained into the previously computed steps, thus this approach relates the next step approximation to a set of the previously computed steps. On the contrary, a one-step scheme evolves the solution toward the next step using only the information coming from the current time approximation. In the framework of one-step schemes family an equivalent accurate approximation can be obtained by means of a multi-stage approach as the one due to Runge and Kutta. FOODIE provides schemes belonging to both these families.

The other ODE solvers classification concerns with explicit or implicit nature of the schemes employed. Briefly, an explicit scheme computes the next step approximation using the previously computed steps at most to the current time, whereas an implicit scheme uses also the next step approximation (that is the unknown), thus it requires extra computations. The implicit approach is of practical use for \emph{stiff} systems where the usage of explicit schemes could require an extremely small time step to evolve in a \emph{stable} way the solution. Mixing together explicit and implicit schemes it is possible to build families of \emph{predictor-corrector} methods: using an explicit scheme to predict a guess for the next step approximation it is possible to use an implicit method for correcting this guess. FOODIE provides explicit, implicit and predictor-correct solvers.

FOODIE currently implements the following ODE schemes:

\begin{itemize}
   \item explicit schemes:
   \begin{itemize}
      \item forward Euler: $1^{st}$ order of accurate;
      \item Adams-Bashforth: multi-step from $1^{st}$ to $16^{th}$ order of accuracy;
      \item Leapfrog:
      \begin{itemize}
         \item unfiltered leapfrog, $2^{nd}$ order accurate, mostly unstable;
         \item Robert-Asselin filtered leapfrog, $1^{st}$ order accurate;
         \item Robert-Asselin-Williams filtered leapfrog, $3^{rd}$ order accurate;
      \end{itemize}
      \item Strong Stability Preserving (SSP) Linear Multistep Methods:
      \begin{itemize}
         \item 3 steps, $2^{nd}$ order accurate;
         \item 4 steps, $3^{rd}$ order accurate;
         \item 5 steps, $3^{rd}$ order accurate;
      \end{itemize}
      \item Strong Stability Preserving (SSP) Linear Multistep Methods with variable step-size:
      \begin{itemize}
         \item from 2 to 3 steps with $2^{nd}$ order of accuracy;
         \item from 3 to 5 steps with $3^{rd}$ order of accuracy;
      \end{itemize}
      \item Strong Stability Preserving (SSP) Linear Multistep Runge-Kutta Methods:
      \begin{itemize}
         \item 2 steps, 2 stages, $3^{rd}$ order accurate;
         \item 3 steps, 2 stages, $3^{rd}$ order accurate;
         \item 4 steps, 5 stages, $8^{th}$ order accurate;
      \end{itemize}
      \item Runge-Kutta schemes:
      \begin{itemize}
         \item Linear Strong Stability Preserving (SSP) RK of any order:
         \begin{itemize}
            \item generic s-stages of order $(s-1)^{th}$;
            \item generic s-stages of order $s^{th}$;
         \end{itemize}
         \item Strong Stability Preserving (SSP) Low Storage RK:
         \begin{itemize}
            \item 5, 6, 7, 12-14 stages, $4^{th}$ order accurate, 2 registers;
         \end{itemize}
         \item Strong Stability Preserving (SSP) RK:
         \begin{itemize}
            \item 2 stages, $2^{nd}$ order accurate;
            \item 3 stages, $3^{rd}$ order accurate;
            \item 5 stages, $4^{th}$ order accurate;
         \end{itemize}
         \item embedded (adaptive) RK:
         \begin{itemize}
            \item Heun-Euler, 2 stages, $2^{nd}$ order accurate;
            \item Runge-Kutta-Cash-Karp, 6 stages, $5^{th}$ order accurate;
            \item Prince-Dormand, 7 stages, $4^{th}$ order accurate;
            \item Calvo, 9 stages, $6^{th}$ order accurate;
            \item Feagin, 17 stages, $10^{th}$ order accurate;
         \end{itemize}
      \end{itemize}
   \end{itemize}
   \item implicit schemes:
   \begin{itemize}
      \item Adams-Moulton: multi-step from $1^{st}$ to $16^{th}$ order of accuracy;
      \item Backward Differentiation Formula: multi-step from $2^{nd}$ to $6^{th}$ order of accuracy;
   \end{itemize}
   \item predictor-corrector schemes:
   \begin{itemize}
      \item Adams-Bashforth-Moulton: multi-step from $1^{st}$ to $16^{th}$ order of accuracy;
   \end{itemize}
\end{itemize}

{\color{red} Add citations to all solvers reference papers.}

In the subsequent sections each of the above mentioned scheme is briefly described.

It is worth noting that for multi-step methods like Adams-Bashforth, Adams-Moulton, Linear Multistep Methods, etc... classes the solvers are not \emph{self-starting}: the values of $U\left(t^{1}\right)$, $U\left(t^{2}\right)$, \dots, $U\left(t^{N_s-1}\right)$ must be provided, $N_s$ being the number of previous steps used. To this aim, a lower order multi-step scheme or an equivalent order one-step multi-stage scheme can be used.

\subsection{Explicit forward Euler scheme}

The explicit forward Euler scheme for ODE integration is probably the simplest solver ever devised. Considering the system \ref{eq:IVP}, the solution (approximation) of the state vector $U$ at the time $t^{n+1}=t^n+\Delta t$ assuming to known the solution at time $t^n$ is:

\begin{equation}
  U\left(t^{n+1}\right) = U\left(t^n\right) +\Delta t \cdot R\left(t^n, U\left(t^n\right)\right)
\label{eq:solver-euler}
\end{equation}
where the solution at the new time step is computed by means of only the current time solution, thus this is an explicit scheme. The solution is an approximation of $1^{st}$ order, the local truncation error being $O(Dt^2)$. As well known, this scheme has an absolute (linear) stability locus equals to $|1+\Delta t \lambda|\le 1$ where $\lambda$ contains the eigenvalues of the linear (or linearized) Jacobian matrix of the system.

This scheme is Total Variation Diminishing (TVD), thus satisfies the maximum principle or the equivalent positivity preserving property. In different fields this property is also indicated as Strong Stability Preserving.

{\color{red} Add citations.}

\subsection{Explicit Adams-Bashforth class}

Adams-Bashforth methods belong to the more general (linear) explicit multi-step family of schemes. This kind of schemes has been designed to achieve a more accurate solution than the $1^{st}$ Euler scheme using the information coming from the solutions already computed at previous time steps. Typically only one new residuals function $R$ evaluation is required at each time step, whereas Runge-Kutta schemes require many of them.

In general, the Adams-Bashforth schemes provided by FOODIE library are written by means of the following algorithm (for only explicit schemes):

\begin{equation}
U\left(t^{n+N_s}\right) = U\left(t^{n+N_s-1}\right) +\Delta t \sum_{s=1}^{n+N_s}{ b_s \cdot R\left[t^{n+s-1}, U\left(t^{n+s-1}\right)\right]}
\label{eq:AB}
\end{equation}
where $N_s$ is the number of time steps considered and $b_s$ are the linear coefficients selected.

Currently FOODIE provides schemes having $2^{nd}$ to $16^{th}$ formal order of accuracy. The $b_s$ coefficients are reported in table \ref{tab:AB-coeff}.

It is worth noting that FODDIE also provides a one-step AB solver that reverts back to the explicit forward Euler scheme: it can be used, for example, into a Recursive Order Reduction (ROR) framework that automatically checks some properties of the solution and, in case, reduces the order of the Runge-Kutta solver until those properties are obtained.

{\color{red} Add citations.}

\subsection{Leapfrog solver}

\emph{Leapfrog} scheme belongs to the multi-step family, it being formally a centered second order approximation in time. The leapfrog method (in its original formulation) is mostly unstable, however it is well suited for periodic-oscillatory problems providing a null error on the amplitude value and a formal second order error on the phase one, under the satisfaction of the time-step size stable limit. Commonly, the leapfrog methods are said to provide a $2 \Delta t$ computational mode that can generate unphysical, unstable solutions. As consequence, the original leapfrog scheme is generally \emph{filtered} in order to suppress these computational modes.

The unfiltered leapfrog scheme provided by FOODIE is:
\begin{equation}
  U\left(t^{n+2}\right) = U\left(t^{n}\right) + 2\Delta t \cdot R\left[t^{n+1}, U\left(t^{n+1}\right)\right]
\label{eq:leapfrog}
\end{equation}

FOODIE provides, in a \emph{seamless} API, also filtered leapfrog schemes. A widely used filter is due to Robert and Asselin, that suppress the computational modes at the cost of accuracy reduction resulting into a $1^{st}$ order error in amplitude value. A more accurate filter, able to provide a $3^{rd}$ order error on amplitude, is a modification of the Robert-Asselin filter due to Williams known as Robert-Asselin-Williams (RAW) filter, that filters the approximation of $U\left(t^{n+1}\right)$ and $U\left(t^{n+2}\right)$ by the following scalar coefficient:

\begin{equation}
  \begin{matrix}
    U\left(t^{n+1}\right) = U\left(t^{n+1}\right) + \Delta * \alpha     \\
    U\left(t^{n+2}\right) = U\left(t^{n+2}\right) + \Delta * (\alpha-1) \\
    where \\
    \Delta = \frac{\nu}{2}(U^{n} - 2 U^{n+1} + U^{n+2})
  \end{matrix}
\label{eq:leapfrog-RAW}
\end{equation}

The filter coefficients should be taken as $\nu \in (0,1]$ and $\alpha \in (0.5,1]$. If $\alpha=0.5$ the filters of time $t^{n+1}$ and $t^{n+2}$ have the same amplitude and opposite sign thus allowing to the optimal $3^{rd}$ order error on amplitude. The default values of the FOODIE provided scheme are $\nu=0.01$ $\alpha=0.53$, but they can be customized at runtime.

{\color{red} Add citations.}

\subsection{Explicit SSP Linear Mutistep Methods class}

Explicit SSP Linear Mutistep Methods belong to multi-step schemes like the Adams-Bashforth ones, but they ensure the Strong Stability Preserving property.

The algorithm implemented into FOODIE is:

\begin{equation}
U^{n+N_s} = \sum_{s=1}^{N_s}{\left[a_s U^{n+s-1} + \Delta t b_s \cdot R(t^{n+s-1}, U^{n+s-1}) \right]}
\label{eq:lmm-ssp}
\end{equation}
where $N_s$ is the number of time steps considered and $a_s, \quad b_s$ are the linear coefficients selected.

Currently FOODIE provides schemes having $2^{nd}$ to $3^{th}$ formal order of accuracy. The $a_s , \quad b_s$ coefficients are reported in table \ref{tab:lmm-ssp-coeff}.

{\color{red} Add citations.}

\subsection{Explicit SSP Linear Mutistep Methods with variable step-size class}

Explicit SSP Linear Mutistep Methods with variable step-size belong to multi-step schemes like the Adams-Bashforth ones, but they ensure the Strong Stability Preserving property and they allow the time step-size to vary, as it happens for multi-stage scheme.

Currently FOODIE provides schemes having $2^{nd}$ to $3^{th}$ formal order of accuracy. The $2^{nd}$ order algorithm implemented into FOODIE is:

\begin{equation}
U^{n+N_s} = \frac{1}{\Omega_{N_s-1}^2} U^n + \frac{\Omega_{N_s-1}^2 - 1}{\Omega_{N_s-1}^2} U^{n+N_s-1} + \frac{\Omega_{N_s-1} + 1}{\Omega_{N_s-1}} \Delta t^{n+N_s} R(U^{n+N_s-1})
\label{eq:lmm-ssp-vss-2}
\end{equation}
while the $3^{rd}$ order algorithm is:
\begin{equation}
U^{n+N_s} = \frac{3 \Omega_{N_s-1} + 2}{\Omega_{N_s-1}^3} U^n +
\frac{(\Omega_{N_s-1} + 1)^2(\Omega_{N_s-1} - 2)}{\Omega_{N_s-1}^3} U^{n+N_s-1} +
\frac{\Omega_{N_s-1} + 1}{\Omega_{N_s-1}^2} \Delta t^{n+N_s} R(U^n) +
\frac{(\Omega_{N_s-1} + 1)^2}{\Omega_{N_s-1}^2} \Delta t^{n+N_s} R(U^{n+N_s-1})
\label{eq:lmm-ssp-vss-3}
\end{equation}

The coefficients are computed by recursive formula:

$$ \Omega_s = \sum_{i=1}^s { \omega_i }\quad 1 \leq s \leq N_s $$
$$ \omega_i = \frac{\Delta t^{n + s}}{\Delta t^{n + N_s}} $$
where $N_s$ is the number of time steps considered.

{\color{red} Add citations.}

\subsection{Explicit SSP Linear Mutistep Methods Runge-Kutta class}

Because SSP Runge-Kutta has an order barrier, see \cite{ketch}, namely they can be at most of $4^{th}$ order accurate, multi-step/multi-stage schemes have been devised. Such a class of schemes represent a good compromise between memory efficiency and accuracy. The schemes are not self-starting, being a multi-step class, but the same multi-step nature allows to overcome the order barrier of SSP multi-stage schemes.

The algorithm implemented into FOODIE is:
\begin{equation}
\begin{matrix}
y_1^n = u^n \\
y_i^n = \sum_{l=1}^{k} d_{il} u^{n-k+l} + \Delta{t}\sum_{l=1}^{k-1} \hat{a}_{il} F(u^{n-k+l}) + \Delta{t}\sum_{j=1}^{i-1} a_{ij} F(y_j^n) \; \; \; \;  2 \leq i \leq N_s \\
   u^{n+1} = \sum_{l=1}^{k} \theta_l u^{n-k+l} + \Delta{t}\sum_{l=1}^{k-1} \hat{b}_{l} F(u^{n-k+l}) + \Delta{t}\sum_{j=1}^{N_s} b_j F(y_j^n).
\end{matrix}
\label{eq:ms-rk-ssp}
\end{equation}
where $N_s$ is the number of time steps considered.

{\color{red} Add citations.}

\subsection{Explicit SSP Runge-Kutta class}

Runge-Kutta methods belong to the more general multi-stage family of schemes. This kind of schemes has been designed to achieve a more accurate solution than the $1^{st}$ Euler scheme, but without increasing the number of time steps used, as it is done with the multi-step schemes. Essentially, the high order of accuracy is obtained by means of \emph{intermediate values} (the stages) of the solution and its derivative are generated and used within a single time step. This commonly implies the allocation of some auxiliary memory registers for storing the intermediate stages.

Notably, the multi-stage schemes class has the attractive property to be \emph{self-starting}: the high order accurate solution can be obtained directly from the previous one, without the necessity to compute \emph{before} a certain number of previous steps, as it happens for the multi-step schemes. Moreover, one-step multi-stage methods are suitable for adaptively-varying time-step size (that is also possible for multi-step schemes, but at a cost of more complexity) and for discontinuous solutions, namely discontinued solutions happening at a certain time $t*$ (that in a multi-step framework can involve an overall accuracy degradation).

In general, the SSP Runge-Kutta schemes provided by FOODIE library are written by means of the following algorithm:
\begin{equation}
  U^{n+1} = U^{n} + \Delta t \cdot\sum\limits_{s=1}^{N_{s}}{\beta_s K_s}
\label{eq:RK}
\end{equation}
where $N_s$ is the number of Runge-Kutta stages used and $K_s$ is the $s^{th}$ stage defined as:
\begin{equation}
  K_s = R\left(t^n + \gamma_s \Delta t, U^n+\Delta t\sum\limits_{l=1}^{s-1}{\alpha_{s,l} K_l} \right)
\label{eq:RK-stage}
\end{equation}
It is worth noting that the equations \ref{eq:RK} and \ref{eq:RK-stage} contain also implicit schemes. A scheme belonging to this family is operative once the coefficients $\alpha,\, \beta,\, \gamma$ are provided. We represent these coefficients using the Butcher's table, that for an explicit scheme where $\gamma_1=\alpha_{1,*}=\alpha_{i,i}=0$ has the form reported in table \ref{tab:butcher-table}.

\begin{table}[!ht]
\caption{Butcher's table for explicit Runge-Kutta schemes}\label{tab:butcher-table}
\centering
\resizebox{0.40\textwidth}{!}{%
\begin{tabular}{c|ccccc}
 $\gamma_2$     & $\alpha_{2,1}$   &                  &          &                      &              \\
 $\gamma_3$     & $\alpha_{3,1}$   & $\alpha_{3,2}$   &          &                      &              \\
 $\vdots$       & $\vdots$         &                  & $\ddots$ &                      &              \\
 $\gamma_{N_s}$ & $\alpha_{N_s,1}$ & $\alpha_{N_s,2}$ & $\cdots$ & $\alpha_{N_s,N_s-1}$ &              \\
 \hline
                & $\beta_1$        & $\beta_2$        & $\cdots$ & $\beta_{N_s-1}$      & $\beta_{N_s}$\\
\end{tabular}}
\end{table}

The equations \ref{eq:RK} and \ref{eq:RK-stage} show that Runge-Kutta methods do not require any additional differentiations of the ODE system for achieving high order accuracy, rather they require additional evaluations of the residuals function $R$.

The nature of the scheme and the properties of the solutions obtained depend on the number of stages and on the value of the coefficients selected. Currently, FOODIE provides many Runge-Kutta schemes having Strong Stability Preserving propriety (thus they being suitable for ODE systems involving rapidly changing non linear dynamics) the Butcher's coefficients of which are reported in tables \ref{tab:RK-TVD-butcher-2}, \ref{tab:RK-SSP-butcher-3}, \ref{tab:RK-SSP-butcher-5}.

The absolute stability locus depends on the coefficients selected, however, as a general principle, we can assume that greater is the stages number and wider is the stability locus on equal accuracy orders. Currently, FOODIE provides $2^{nd}$ to $4^{th}$ order accurate schemes. As demonstrated in \cite{ketch}, SSP RK are at most $4^{th}$ order accurate, namely the SSP property introduces an order barrier for single step methods. To increase the order it is necessary to adopt multi-step schemes: the hybrid multi-step RK schemes are devised to this aim.

Similarly to the Adams-Bashforth class, the Runge-Kutta class also provides a fail-safe one-stage solver reverting back to the explicit forward Euler solver, that is useful for ROR-like frameworks.

{\color{red} Add citations.}

\subsection{Explicit low storage Runge-Kutta class}

As aforementioned, standard Runge-Kutta schemes have the drawback to require $N_S$ auxiliary memory registers to store the necessary stages data. In order to make an efficient use of the available limited computer memory, the class of low storage Runge-Kutta scheme was devised. Essentially, the standard Runge-Kutta class (under some specific conditions) can be reformulated allowing a more efficient memory management. Currently FOODIE provides a class of \emph{2N registers} storage Runge-Kutta schemes, meaning that the storage of all stages requires only 2 registers of memory with a \emph{word} length $N$ (namely the length of the state vector) in contrast to the standard formulation where $N_s$ registers of the same length $N$ are required. This is a dramatic improvement of memory efficiency especially for schemes using a high number of stages ($N_s \ge 4$) where the memory necessary is an half with respect the original formulation. Unfortunately, not all standard Runge-Kutta schemes can be reformulated as a low storage one.

Following the Williamson's approach the standard coefficients are reformulated to the coefficients vectors $A$, $B$ and $C$ and the Runge-Kutta algorithm becomes:

\begin{equation}
\begin{matrix}
  K_1 = U\left(t^n\right) \\
  K_2 = 0 \\
  \left.\begin{matrix}
    K_2 = A_s K_2 + \Delta t \cdot R\left(t^n + C_s \Delta t, K_1\right) \\
    K_1 = K_1 + B_s K_2
  \end{matrix}\right\} s=1,2,...N_s\\
  U\left(t^{n+1}\right) = K_1
  \end{matrix}
\label{eq:RK-ls}
\end{equation}

Currently FOODIE provides 5, 6, 7, 12, 13 and 14 stages, all $4^{th}$ order, 2N registers explicit schemes, the coefficients of which are listed in table \ref{tab:RK-ls}.

Similarly to the SSP Runge-Kutta class, the low storage class also provides a fail-safe one-stage solver reverting back to the explicit forward Euler solver, that is useful for ROR-like frameworks.

{\color{red} Add citations.}

\subsection{Implicit Adams-Moulton class}

Adams-Moulton methods belong to the more general (linear) implicit multi-step family of schemes. This kind of schemes has been designed to achieve a more accurate solution than the $1^{st}$ Euler scheme using the information coming from the solutions already computed at previous time steps. Typically only one new residuals function $R$ evaluation is required at each time step, whereas Runge-Kutta schemes require many of them.

In general, the Adams-Moulton schemes provided by FOODIE library are written by means of the following algorithm (for only implicit schemes):

\begin{equation}
  U\left(t^{n+N_s}\right) = U\left(t^{n+N_s-1}\right) +\Delta t \sum_{s=0}^{n+N_s-1}{ b_s \cdot R\left[t^{n+s}, U\left(t^{n+s}\right)\right]} + b_{N_s}\cdot R\left[t^{n+N_s}, U\left(t^{n+N_s}\right)\right]
\label{eq:AM}
\end{equation}
where $N_s$ is the number of time steps considered and $b_s$ are the linear coefficients selected.

Currently FOODIE provides schemes having $2^{nd}$ to $16^{th}$ formal order of accuracy. The $b_s$ coefficients are reported in table \ref{tab:AM-coeff}.

Similarly to the Runge-Kutta and Adams-Bashforth classes, the Adams-Moulton class also provides a fail-safe zero-step solver reverting back to the implicit backward Euler solver, that is useful for ROR-like frameworks.

{\color{red} Add citations.}

\subsection{Predictor-corrector Adams-Bashforth-Moulton class}

Adams-Bashforth-Moulton methods belong to the more general (linear) predictor-corrector multi-step family of schemes. This kind of schemes has been designed to achieve a more accurate solution than the $1^{st}$ Euler scheme using the information coming from the solutions already computed at previous time steps. Typically only one new residuals function $R$ evaluation is required at each time step, whereas Runge-Kutta schemes require many of them.

In general, the Adams-Bashforth-Moulton schemes provided by FOODIE library are written by means of the following algorithm:

\begin{equation}
  \begin{matrix}
    U\left(t^{n+N_s^p}\right)_p = U\left(t^{n+N_s^p-1}\right) +\Delta t \sum_{s=1}^{n+N_s^p}{ b_s^p \cdot R\left[t^{n+s-1}, U\left(t^{n+s-1}\right)\right]}   \\
    U\left(t^{n+N_s^c}\right)_c = U\left(t^{n+N_s^c-1}\right) +\Delta t \sum_{s=0}^{n+N_s^c-1}{ b_s^c \cdot R\left[t^{n+s}, U\left(t^{n+s}\right)\right]} + b_{N_s^c}^c\cdot R\left[t^{n+N_s^p}, U\left(t^{n+N_s^p}\right)_p\right]
  \end{matrix}
\label{eq:ABM}
\end{equation}
where $N_s^{p,c}$ is the number of time steps considered for the Adams-Bashforth predictor/Adams-Moulton corrector (respectively) and $b_s^{p,c}$ are the corresponding linear coefficients selected. Essentially, the Adams-Bashforth prediction $U\left(t^{n+N_s^p}\right)_p$ is corrected by means of the Adams-Moulton correction resulting in $U\left(t^{n+N_s^c}\right)_c$. In order to preserve the formal order of accuracy the relation $N_s^p=N_s^c+1$ always holds.

Currently FOODIE provides schemes having $2^{nd}$ to $16^{th}$ formal order of accuracy. The $b_s^{p,c}$ coefficients are those reported in tables \ref{tab:AB-coeff} and \ref{tab:AM-coeff}.

{\color{red} Add citations.}
